"""
ë¬¸ì„œ ë§¤í•‘ ìƒì„± í†µí•© ë„êµ¬
- PromptManagerë¥¼ ì‚¬ìš©í•˜ì—¬ YAML ê¸°ë°˜ LLM í”„ë¡¬í”„íŠ¸ ìƒì„±
- LLM API í˜¸ì¶œ ë° ë§¤í•‘ ê²°ê³¼ íŒŒì‹±
"""

import json
from typing import Dict, List, Any, Optional
import anthropic  # Anthropic SDK


def parse_mapping_response(response_text: str) -> Dict[str, Any]:
    """
    LLM ì‘ë‹µì—ì„œ JSON ë§¤í•‘ ì •ë³´ ì¶”ì¶œ

    Args:
        response_text: LLMì˜ ì‘ë‹µ í…ìŠ¤íŠ¸

    Returns:
        íŒŒì‹±ëœ ë§¤í•‘ ë”•ì…”ë„ˆë¦¬
    """
    # JSON ì½”ë“œ ë¸”ë¡ì—ì„œ ì¶”ì¶œ
    import re

    # ```json ... ``` ë˜ëŠ” ``` ... ``` í˜•ì‹ ì°¾ê¸°
    json_match = re.search(r'```(?:json)?\s*(\{.*?\})\s*```', response_text, re.DOTALL)

    if json_match:
        json_str = json_match.group(1)
    else:
        # ì½”ë“œ ë¸”ë¡ì´ ì—†ìœ¼ë©´ ì „ì²´ í…ìŠ¤íŠ¸ì—ì„œ JSON ì°¾ê¸°
        json_match = re.search(r'\{.*\}', response_text, re.DOTALL)
        if json_match:
            json_str = json_match.group(0)
        else:
            raise ValueError("ì‘ë‹µì—ì„œ JSONì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    try:
        mappings = json.loads(json_str)
        return mappings
    except json.JSONDecodeError as e:
        raise ValueError(f"JSON íŒŒì‹± ì‹¤íŒ¨: {e}")


def convert_mapping_to_array(
    mappings: Dict[str, Any],
    source_documents: List[Dict[str, Any]],
    processed_document: Dict[str, Any]
) -> List[Dict[str, Any]]:
    """
    generated_doc_to_source_doc í˜•ì‹ì˜ ë§¤í•‘ì„ mappings ë°°ì—´ë¡œ ë³€í™˜

    Args:
        mappings: generated_doc_to_source_doc, source_doc_to_generated_doc í¬í•¨í•œ ë§¤í•‘ ë”•ì…”ë„ˆë¦¬
        source_documents: ì†ŒìŠ¤ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
        processed_document: ìƒì„±ëœ ë¬¸ì„œ

    Returns:
        mappings ë°°ì—´ (ê° í•­ëª©: source_sentence, target_sentence, type, confidence ë“±)
    """
    result = []

    # ë¬¸ì¥ ID â†’ ë¬¸ì¥ í…ìŠ¤íŠ¸ ë§¤í•‘ ìƒì„±
    source_sentences = {}
    for doc in source_documents:
        for sent in doc.get('content', {}).get('sentences', []):
            source_sentences[sent['id']] = sent['text']

    target_sentences = {}
    for sent in processed_document.get('content', {}).get('sentences', []):
        target_sentences[sent['id']] = sent['text']

    # generated_doc_to_source_docë¥¼ mappings ë°°ì—´ë¡œ ë³€í™˜
    generated_to_source = mappings.get('generated_doc_to_source_doc', {})
    for target_id, source_ids in generated_to_source.items():
        target_text = target_sentences.get(target_id, '')

        for source_id in source_ids:
            source_text = source_sentences.get(source_id, '')

            if source_text and target_text:
                result.append({
                    'source_id': source_id,
                    'target_id': target_id,
                    'source_sentence': source_text,
                    'target_sentence': target_text,
                    'type': 'summary',
                    'confidence': 85  # ê¸°ë³¸ê°’
                })

    return result


def generate_mappings_with_anthropic(
    source_documents: List[Dict[str, Any]],
    processed_document: Dict[str, Any],
    api_key: str,
    existing_mappings: Optional[Dict[str, Any]] = None,
    model: Optional[str] = None,
    processing_type: str = "generic",
    original_texts: Optional[Dict[str, str]] = None
) -> Dict[str, Any]:
    """
    Anthropic SDKë¥¼ ì‚¬ìš©í•˜ì—¬ ë¬¸ì„œ ê°„ ë§¤í•‘ ìƒì„±

    Args:
        source_documents: ì†ŒìŠ¤ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
        processed_document: ì²˜ë¦¬ëœ ë¬¸ì„œ
        api_key: Anthropic API í‚¤
        existing_mappings: ê¸°ì¡´ ë§¤í•‘ (ì„ íƒ)
        model: ì‚¬ìš©í•  ëª¨ë¸ (ê¸°ë³¸: claude-3-5-sonnet-20241022)
        processing_type: ì²˜ë¦¬ ìœ í˜• (summary, translation, paraphrase ë“±)
        original_texts: ì›ë³¸ í…ìŠ¤íŠ¸ ë”•ì…”ë„ˆë¦¬ (ì„ íƒ)

    Returns:
        ë§¤í•‘ ì •ë³´ ë”•ì…”ë„ˆë¦¬
    """
    # ê¸°ë³¸ ëª¨ë¸ ì„¤ì •
    if model is None:
        model = "claude-3-5-sonnet-20241022"

    # PromptManagerë¥¼ ì‚¬ìš©í•˜ì—¬ YAML í…œí”Œë¦¿ ë Œë”ë§
    from app.core.prompts.prompt_manager import get_prompt_manager

    prompt_manager = get_prompt_manager()

    # ë¬¸ì„œ êµ¬ì¡°í™”ëœ ë°ì´í„° ì¤€ë¹„ (Jinja2 í…œí”Œë¦¿ ë³€ìˆ˜)
    template_variables = {
        'structured_source_docs': source_documents,
        'structured_generated_doc': processed_document,
        'existing_mappings': existing_mappings
    }

    # YAML í…œí”Œë¦¿ ë Œë”ë§
    prompt = prompt_manager.render(
        yaml_file='mapping_prompts.yaml',
        task_id='document_mapping_generation',
        variables=template_variables
    )

    client = anthropic.Anthropic(api_key=api_key)

    message = client.messages.create(
        model=model,
        max_tokens=4096,
        messages=[{"role": "user", "content": prompt}]
    )

    response_text = message.content[0].text
    mappings = parse_mapping_response(response_text)

    return mappings


def generate_mappings_with_requests(
    structured_source_docs: List[Dict[str, Any]],
    structured_processed_doc: Dict[str, Any],
    api_key: str,
    existing_mappings: Optional[Dict[str, Any]] = None,
    model: str = "claude-3-5-sonnet-20241022",
    provider: str = "anthropic",
    processing_type: str = "generic",
    original_texts: Optional[Dict[str, str]] = None
) -> Dict[str, Any]:
    """
    HTTP requestsë¥¼ ì‚¬ìš©í•˜ì—¬ ë¬¸ì„œ ê°„ ë§¤í•‘ ìƒì„± (SDK ì—†ì´)

    Args:
        structured_source_docs: êµ¬ì¡°í™”ëœ ì†ŒìŠ¤ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
        structured_processed_doc: êµ¬ì¡°í™”ëœ ì²˜ë¦¬ ë¬¸ì„œ
        api_key: API í‚¤
        existing_mappings: ê¸°ì¡´ ë§¤í•‘ ì •ë³´
        model: ëª¨ë¸ëª…
        provider: "anthropic" ë˜ëŠ” "openai"
        processing_type: ì²˜ë¦¬ ìœ í˜• (ì„ íƒ)
        original_texts: ì›ë³¸ í…ìŠ¤íŠ¸ ë”•ì…”ë„ˆë¦¬ (ì„ íƒ)

    Returns:
        ìƒì„±ëœ ë§¤í•‘ ì •ë³´
    """
    import requests

    # PromptManagerë¥¼ ì‚¬ìš©í•˜ì—¬ YAML í…œí”Œë¦¿ ë Œë”ë§
    from app.core.prompts.prompt_manager import get_prompt_manager

    prompt_manager = get_prompt_manager()

    # ë¬¸ì„œ êµ¬ì¡°í™”ëœ ë°ì´í„° ì¤€ë¹„ (Jinja2 í…œí”Œë¦¿ ë³€ìˆ˜)
    template_variables = {
        'structured_source_docs': structured_source_docs,
        'structured_generated_doc': structured_processed_doc,
        'existing_mappings': existing_mappings
    }

    # YAML í…œí”Œë¦¿ ë Œë”ë§
    prompt = prompt_manager.render(
        yaml_file='mapping_prompts.yaml',
        task_id='document_mapping_generation',
        variables=template_variables
    )
    
    if provider == "anthropic":
        url = "https://api.anthropic.com/v1/messages"
        headers = {
            "x-api-key": api_key,
            "anthropic-version": "2023-06-01",
            "content-type": "application/json"
        }
        payload = {
            "model": model,
            "max_tokens": 4096,
            "messages": [
                {"role": "user", "content": prompt}
            ]
        }
        
        response = requests.post(url, headers=headers, json=payload)
        response.raise_for_status()
        
        result = response.json()
        response_text = result["content"][0]["text"]
        
    elif provider == "openai":
        url = "https://api.openai.com/v1/chat/completions"
        headers = {
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json"
        }
        payload = {
            "model": model or "gpt-4",
            "messages": [
                {"role": "system", "content": "ë‹¹ì‹ ì€ ë¬¸ì„œ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤."},
                {"role": "user", "content": prompt}
            ],
            "temperature": 0.1
        }
        
        response = requests.post(url, headers=headers, json=payload)
        response.raise_for_status()
        
        result = response.json()
        response_text = result["choices"][0]["message"]["content"]
        
    else:
        raise ValueError(f"Unknown provider: {provider}")
    
    mappings = parse_mapping_response(response_text)
    return mappings


# í•˜ìœ„ í˜¸í™˜ì„±ì„ ìœ„í•œ ë³„ì¹­
def generate_mappings_with_llm(
    structured_source_docs: List[Dict[str, Any]],
    structured_processed_doc: Dict[str, Any],
    api_key: str,
    existing_mappings: Optional[Dict[str, Any]] = None,
    model: str = "claude-3-5-sonnet-20241022"
) -> Dict[str, Any]:
    """
    LLMì„ ì‚¬ìš©í•˜ì—¬ ë¬¸ì„œ ê°„ ë§¤í•‘ ìƒì„± (Anthropic ê¸°ë³¸)
    """
    return generate_mappings_with_anthropic(
        structured_source_docs,
        structured_processed_doc,
        api_key,
        existing_mappings,
        model
    )


def validate_mappings(
    mappings: Dict[str, Any],
    structured_source_docs: List[Dict[str, Any]],
    structured_processed_doc: Dict[str, Any]
) -> Dict[str, List[str]]:
    """
    ë§¤í•‘ì˜ ìœ íš¨ì„± ê²€ì¦
    
    Args:
        mappings: ê²€ì¦í•  ë§¤í•‘ ì •ë³´
        structured_source_docs: ì†ŒìŠ¤ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
        structured_processed_doc: ì²˜ë¦¬ ë¬¸ì„œ
    
    Returns:
        ê²€ì¦ ê²°ê³¼ (errorsì™€ warnings)
    """
    errors = []
    warnings = []
    
    # ëª¨ë“  ë¬¸ì¥ ID ìˆ˜ì§‘
    source_sentence_ids = set()
    for doc in structured_source_docs:
        for sent in doc['content']['sentences']:
            source_sentence_ids.add(sent['id'])
    
    processed_sentence_ids = set(
        s['id'] for s in structured_processed_doc['content']['sentences']
    )
    
    # generated_doc_to_source_doc ê²€ì¦
    for proc_id, source_ids in mappings.get('generated_doc_to_source_doc', {}).items():
        if proc_id not in processed_sentence_ids:
            errors.append(f"ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ì²˜ë¦¬ ë¬¸ì„œ ë¬¸ì¥ ID: {proc_id}")
        
        for source_id in source_ids:
            if source_id not in source_sentence_ids:
                errors.append(f"ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ì†ŒìŠ¤ ë¬¸ì¥ ID: {source_id}")
    
    # source_doc_to_generated_doc ê²€ì¦
    for source_id, proc_ids in mappings.get('source_doc_to_generated_doc', {}).items():
        if source_id not in source_sentence_ids:
            errors.append(f"ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ì†ŒìŠ¤ ë¬¸ì¥ ID: {source_id}")
        
        for proc_id in proc_ids:
            if proc_id not in processed_sentence_ids:
                errors.append(f"ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ì²˜ë¦¬ ë¬¸ì„œ ë¬¸ì¥ ID: {proc_id}")
    
    # ì–‘ë°©í–¥ ì¼ê´€ì„± ê²€ì¦
    for proc_id, source_ids in mappings.get('generated_doc_to_source_doc', {}).items():
        for source_id in source_ids:
            reverse_mapping = mappings.get('source_doc_to_generated_doc', {}).get(source_id, [])
            if proc_id not in reverse_mapping:
                warnings.append(
                    f"ì–‘ë°©í–¥ ë¶ˆì¼ì¹˜: {proc_id} â†’ {source_id}ëŠ” ìˆì§€ë§Œ ì—­ë°©í–¥ ë§¤í•‘ ì—†ìŒ"
                )
    
    return {
        "errors": errors,
        "warnings": warnings
    }


# ============================================================================
# ì‚¬ìš© ì˜ˆì‹œ
# ============================================================================

if __name__ == "__main__":
    # ì˜ˆì‹œ ë°ì´í„° (ì´ì „ structure_document í•¨ìˆ˜ë¡œ ìƒì„±ëœ ê²ƒ ê°€ì •)
    source_docs = [
        {
            "doc_id": "src_001",
            "metadata": {"title": "AI ìœ¤ë¦¬ ê°€ì´ë“œë¼ì¸"},
            "content": {
                "sentences": [
                    {"id": "src_001_s1", "text": "AI ìœ¤ë¦¬ ì›ì¹™ì˜ ì¤‘ìš”ì„±", "lines": [1]},
                    {"id": "src_001_s2", "text": "íˆ¬ëª…ì„±ê³¼ ê³µì •ì„±ì´ í•„ìˆ˜ì ì…ë‹ˆë‹¤.", "lines": [2]}
                ]
            }
        }
    ]
    
    processed_doc = {
        "doc_id": "gen_001",
        "metadata": {"title": "AI ìœ¤ë¦¬ ìš”ì•½"},
        "content": {
            "sentences": [
                {"id": "gen_001_s1", "text": "AI ìœ¤ë¦¬ì™€ íˆ¬ëª…ì„±ì´ ì¤‘ìš”í•˜ë‹¤.", "lines": [1]}
            ]
        }
    }

    # 1. í”„ë¡¬í”„íŠ¸ ìƒì„± (PromptManager ì‚¬ìš©)
    from app.core.prompts.prompt_manager import get_prompt_manager

    prompt_manager = get_prompt_manager()
    template_variables = {
        'structured_source_docs': source_docs,
        'structured_generated_doc': processed_doc,
        'existing_mappings': None
    }
    prompt = prompt_manager.render(
        yaml_file='mapping_prompts.yaml',
        task_id='document_mapping_generation',
        variables=template_variables
    )
    print("ìƒì„±ëœ í”„ë¡¬í”„íŠ¸:")
    print(prompt[:500] + "...\n")
    
    # 2. LLMìœ¼ë¡œ ë§¤í•‘ ìƒì„± (API í‚¤ í•„ìš”)
    # mappings = generate_mappings_with_llm(
    #     source_docs,
    #     processed_doc,
    #     api_key="your-api-key"
    # )
    
    # 3. ìˆ˜ë™ìœ¼ë¡œ ì‘ì„±í•œ ë§¤í•‘ ì˜ˆì‹œ
    mappings = {
        "generated_doc_to_source_doc": {
            "gen_001_s1": ["src_001_s1", "src_001_s2"]
        },
        "source_doc_to_generated_doc": {
            "src_001_s1": ["gen_001_s1"],
            "src_001_s2": ["gen_001_s1"]
        }
    }
    
    # 4. ë§¤í•‘ ê²€ì¦
    validation_result = validate_mappings(mappings, source_docs, processed_doc)
    
    print("âœ… ë§¤í•‘ ê²€ì¦ ì™„ë£Œ!")
    if validation_result['errors']:
        print(f"âŒ ì˜¤ë¥˜: {len(validation_result['errors'])}ê°œ")
        for error in validation_result['errors']:
            print(f"  - {error}")
    else:
        print("  ì˜¤ë¥˜ ì—†ìŒ")
    
    if validation_result['warnings']:
        print(f"âš ï¸  ê²½ê³ : {len(validation_result['warnings'])}ê°œ")
        for warning in validation_result['warnings']:
            print(f"  - {warning}")
    else:
        print("  ê²½ê³  ì—†ìŒ")
    
    # 5. ì „ì²´ ë°ì´í„° êµ¬ì¡°ì— ë§¤í•‘ ì¶”ê°€
    full_data = {
        "source_documents": source_docs,
        "processed_document": processed_doc,
        "mappings": mappings
    }
    
    print("\nâœ… ìµœì¢… ë°ì´í„° êµ¬ì¡°:")
    print(json.dumps(full_data, ensure_ascii=False, indent=2))


# ============================================================================
# OpenAI ì‚¬ìš© ë²„ì „ (ì„ íƒ)
# ============================================================================

def generate_mappings_with_openai(
    source_documents: List[Dict[str, Any]],
    processed_document: Dict[str, Any],
    api_key: str,
    existing_mappings: Optional[Dict[str, Any]] = None,
    model: Optional[str] = None,
    processing_type: str = "generic",
    original_texts: Optional[Dict[str, str]] = None
) -> Dict[str, Any]:
    """
    OpenAIë¥¼ ì‚¬ìš©í•˜ì—¬ ë¬¸ì„œ ê°„ ë§¤í•‘ ìƒì„± (ìµœì‹  SDK ì‚¬ìš©)

    ìƒˆë¡œìš´ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì‹œìŠ¤í…œì„ ì‚¬ìš©í•©ë‹ˆë‹¤.

    Args:
        source_documents: ì†ŒìŠ¤ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
        processed_document: ì²˜ë¦¬ëœ ë¬¸ì„œ
        api_key: OpenAI API í‚¤
        existing_mappings: ê¸°ì¡´ ë§¤í•‘ (ì„ íƒ)
        model: ì‚¬ìš©í•  ëª¨ë¸ (ê¸°ë³¸: gpt-4o-mini)
        processing_type: ì²˜ë¦¬ ìœ í˜• (summary, translation, paraphrase ë“±)
        original_texts: ì›ë³¸ í…ìŠ¤íŠ¸ ë”•ì…”ë„ˆë¦¬ (ì„ íƒ)

    Returns:
        ë§¤í•‘ ì •ë³´ ë”•ì…”ë„ˆë¦¬
    """
    from openai import OpenAI

    # ê¸°ë³¸ ëª¨ë¸ ì„¤ì •
    if model is None:
        model = "gpt-4o-mini"

    # PromptManagerë¥¼ ì‚¬ìš©í•˜ì—¬ YAML í…œí”Œë¦¿ ë Œë”ë§
    from app.core.prompts.prompt_manager import get_prompt_manager

    prompt_manager = get_prompt_manager()

    # ë¬¸ì„œ êµ¬ì¡°í™”ëœ ë°ì´í„° ì¤€ë¹„ (Jinja2 í…œí”Œë¦¿ ë³€ìˆ˜)
    template_variables = {
        'structured_source_docs': source_documents,
        'structured_generated_doc': processed_document,
        'existing_mappings': None  # ê¸°ì¡´ ë§¤í•‘ì´ ìˆìœ¼ë©´ ì „ë‹¬ ê°€ëŠ¥
    }

    # YAML í…œí”Œë¦¿ ë Œë”ë§
    prompt = prompt_manager.render(
        yaml_file='mapping_prompts.yaml',
        task_id='document_mapping_generation',
        variables=template_variables
    )

    # OpenAI API í˜¸ì¶œ (ìµœì‹  SDK)
    print(f"ğŸ¤– OpenAI API í˜¸ì¶œ ì‹œì‘ - ëª¨ë¸: {model}")
    print(f"ğŸ“ í”„ë¡¬í”„íŠ¸ ê¸¸ì´: {len(prompt)} ë¬¸ì")

    client = OpenAI(api_key=api_key)

    response = client.chat.completions.create(
        model=model,
        messages=[
            {"role": "system", "content": "ë‹¹ì‹ ì€ ë¬¸ì„œ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. í•­ìƒ ì •í™•í•œ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”."},
            {"role": "user", "content": prompt}
        ],
        temperature=0.1,
        max_tokens=4096
    )

    response_text = response.choices[0].message.content
    print(f"âœ… OpenAI ì‘ë‹µ ë°›ìŒ - ê¸¸ì´: {len(response_text)} ë¬¸ì")
    print(f"ğŸ“„ ì‘ë‹µ ë¯¸ë¦¬ë³´ê¸°: {response_text[:200]}...")

    # ì‘ë‹µ íŒŒì‹±
    mappings = parse_mapping_response(response_text)
    print(f"ğŸ“Š íŒŒì‹± ì™„ë£Œ - generated_doc_to_source_doc: {len(mappings.get('generated_doc_to_source_doc', {}))} í•­ëª©")

    # generated_doc_to_source_doc í˜•ì‹ì„ mappings ë°°ì—´ë¡œ ë³€í™˜
    mappings_array = convert_mapping_to_array(mappings, source_documents, processed_document)
    print(f"ğŸ“Š ë³€í™˜ëœ ë§¤í•‘ ìˆ˜: {len(mappings_array)}")

    # ìµœì¢… í˜•ì‹ìœ¼ë¡œ ë°˜í™˜
    return {
        "mappings": mappings_array,
        "generated_doc_to_source_doc": mappings.get('generated_doc_to_source_doc', {}),
        "source_doc_to_generated_doc": mappings.get('source_doc_to_generated_doc', {})
    }


# ============================================================================
# ë°°ì¹˜ ì²˜ë¦¬ ìœ í‹¸ë¦¬í‹°
# ============================================================================

def update_mapping_single(
    data_file: str,
    output_file: str,
    api_key: str,
    force_regenerate: bool = False,
    llm_provider: str = "anthropic",
    model: Optional[str] = None
) -> bool:
    """
    ë‹¨ì¼ ë¬¸ì„œì˜ ë§¤í•‘ ìƒì„±/ì—…ë°ì´íŠ¸

    Args:
        data_file: ì…ë ¥ JSON íŒŒì¼ ê²½ë¡œ
        output_file: ì¶œë ¥ JSON íŒŒì¼ ê²½ë¡œ
        api_key: LLM API í‚¤
        force_regenerate: ê¸°ì¡´ ë§¤í•‘ì´ ìˆì–´ë„ ì¬ìƒì„±í• ì§€ ì—¬ë¶€
        llm_provider: LLM ì œê³µì (anthropic/openai)
        model: ì‚¬ìš©í•  ëª¨ë¸ëª… (ì„ íƒ)

    Returns:
        ì„±ê³µ ì—¬ë¶€ (True/False)
    """
    # ë°ì´í„° ë¡œë“œ
    with open(data_file, 'r', encoding='utf-8') as f:
        data = json.load(f)

    source_docs = data.get('source_documents', [])
    processed_doc = data.get('processed_document')
    existing_mappings = data.get('mappings') if not force_regenerate else None

    print(f"ğŸ“„ ë¬¸ì„œ ë¡œë“œ: {data_file}")
    print(f"  - ì†ŒìŠ¤ ë¬¸ì„œ: {len(source_docs)}ê°œ")
    print(f"  - ìƒì„± ë¬¸ì„œ: {processed_doc['metadata']['title']}")

    # ë§¤í•‘ ìƒì„±
    print("\nğŸ”„ ë§¤í•‘ ìƒì„± ì¤‘...")
    try:
        if llm_provider == "anthropic":
            mappings = generate_mappings_with_anthropic(
                source_docs,
                processed_doc,
                api_key,
                existing_mappings,
                model
            )
        elif llm_provider == "openai":
            mappings = generate_mappings_with_openai(
                source_docs,
                processed_doc,
                api_key,
                existing_mappings,
                model
            )
        else:
            raise ValueError(f"Unknown provider: {llm_provider}")

        print("âœ… ë§¤í•‘ ìƒì„± ì™„ë£Œ!")
    except Exception as e:
        print(f"âŒ ë§¤í•‘ ìƒì„± ì‹¤íŒ¨: {e}")
        return False

    # ê²€ì¦
    print("\nğŸ” ë§¤í•‘ ê²€ì¦ ì¤‘...")
    validation = validate_mappings(mappings, source_docs, processed_doc)

    if validation['errors']:
        print(f"âŒ ê²€ì¦ ì‹¤íŒ¨: {len(validation['errors'])}ê°œ ì˜¤ë¥˜")
        for error in validation['errors']:
            print(f"  - {error}")
        return False

    if validation['warnings']:
        print(f"âš ï¸  {len(validation['warnings'])}ê°œ ê²½ê³ :")
        for warning in validation['warnings'][:5]:
            print(f"  - {warning}")
        if len(validation['warnings']) > 5:
            print(f"  ... ì™¸ {len(validation['warnings']) - 5}ê°œ")

    # ê²°ê³¼ ì €ì¥
    data['mappings'] = mappings
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

    print(f"âœ… ê²°ê³¼ ì €ì¥: {output_file}")

    # í†µê³„ ì¶œë ¥
    generated_count = len(mappings.get('generated_doc_to_source_doc', {}))
    source_count = len(mappings.get('source_doc_to_generated_doc', {}))
    print(f"ğŸ“Š ë§¤í•‘ í†µê³„: ìƒì„± {generated_count}ê°œ, ì†ŒìŠ¤ {source_count}ê°œ")

    return True


def update_mappings_batch(
    data_files: List[str],
    output_dir: str,
    api_key: str,
    force_regenerate: bool = False,
    llm_provider: str = "anthropic",
    model: Optional[str] = None,
    continue_on_error: bool = True
) -> Dict[str, Any]:
    """
    ì—¬ëŸ¬ ë¬¸ì„œì˜ ë§¤í•‘ì„ ì¼ê´„ ìƒì„±/ì—…ë°ì´íŠ¸ (ì§„ì§œ ë°°ì¹˜ ì²˜ë¦¬)

    Args:
        data_files: ì…ë ¥ JSON íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸
        output_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬ ê²½ë¡œ
        api_key: LLM API í‚¤
        force_regenerate: ê¸°ì¡´ ë§¤í•‘ì´ ìˆì–´ë„ ì¬ìƒì„±í• ì§€ ì—¬ë¶€
        llm_provider: LLM ì œê³µì (anthropic/openai)
        model: ì‚¬ìš©í•  ëª¨ë¸ëª… (ì„ íƒ)
        continue_on_error: ì—ëŸ¬ ë°œìƒ ì‹œ ê³„ì† ì§„í–‰ ì—¬ë¶€

    Returns:
        ë°°ì¹˜ ì²˜ë¦¬ ê²°ê³¼ í†µê³„
    """
    import os
    from pathlib import Path

    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)

    print(f"\n{'='*70}")
    print(f"ğŸš€ ë°°ì¹˜ ë§¤í•‘ ìƒì„± ì‹œì‘")
    print(f"{'='*70}")
    print(f"ğŸ“ ì…ë ¥ íŒŒì¼: {len(data_files)}ê°œ")
    print(f"ğŸ“ ì¶œë ¥ ë””ë ‰í† ë¦¬: {output_dir}")
    print(f"ğŸ¤– LLM ì œê³µì: {llm_provider}")
    if model:
        print(f"ğŸ¯ ëª¨ë¸: {model}")
    print(f"{'='*70}\n")

    # ê²°ê³¼ ì¶”ì 
    results = {
        'total': len(data_files),
        'success': 0,
        'failed': 0,
        'skipped': 0,
        'details': []
    }

    # ê° íŒŒì¼ ì²˜ë¦¬
    for idx, data_file in enumerate(data_files, 1):
        print(f"\n{'â”€'*70}")
        print(f"[{idx}/{len(data_files)}] ì²˜ë¦¬ ì¤‘: {os.path.basename(data_file)}")
        print(f"{'â”€'*70}")

        # ì¶œë ¥ íŒŒì¼ëª… ìƒì„±
        input_name = Path(data_file).stem
        output_file = output_path / f"{input_name}_mapped.json"

        try:
            # ë‹¨ì¼ ë¬¸ì„œ ì²˜ë¦¬
            success = update_mapping_single(
                data_file=data_file,
                output_file=str(output_file),
                api_key=api_key,
                force_regenerate=force_regenerate,
                llm_provider=llm_provider,
                model=model
            )

            if success:
                results['success'] += 1
                results['details'].append({
                    'file': data_file,
                    'status': 'success',
                    'output': str(output_file)
                })
            else:
                results['failed'] += 1
                results['details'].append({
                    'file': data_file,
                    'status': 'failed',
                    'error': 'Validation failed'
                })

                if not continue_on_error:
                    print(f"\nâŒ ì—ëŸ¬ë¡œ ì¸í•´ ë°°ì¹˜ ì²˜ë¦¬ ì¤‘ë‹¨")
                    break

        except Exception as e:
            results['failed'] += 1
            results['details'].append({
                'file': data_file,
                'status': 'failed',
                'error': str(e)
            })
            print(f"âŒ ì˜ˆì™¸ ë°œìƒ: {e}")

            if not continue_on_error:
                print(f"\nâŒ ì—ëŸ¬ë¡œ ì¸í•´ ë°°ì¹˜ ì²˜ë¦¬ ì¤‘ë‹¨")
                break

    # ìµœì¢… ê²°ê³¼ ìš”ì•½
    print(f"\n{'='*70}")
    print(f"ğŸ“Š ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ")
    print(f"{'='*70}")
    print(f"âœ… ì„±ê³µ: {results['success']}/{results['total']}")
    print(f"âŒ ì‹¤íŒ¨: {results['failed']}/{results['total']}")
    if results['skipped'] > 0:
        print(f"â­ï¸  ìŠ¤í‚µ: {results['skipped']}/{results['total']}")
    print(f"{'='*70}\n")

    return results


# ============================================================================
# CLI ì¸í„°í˜ì´ìŠ¤
# ============================================================================

if __name__ == "__main__" and len(__import__('sys').argv) > 1:
    import sys
    import argparse
    import glob

    parser = argparse.ArgumentParser(
        description='ë¬¸ì„œ ë§¤í•‘ ìƒì„± ë„êµ¬ (ë‹¨ì¼/ë°°ì¹˜ ì²˜ë¦¬)',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì‚¬ìš© ì˜ˆì‹œ:
  # ë‹¨ì¼ íŒŒì¼ ì²˜ë¦¬
  python mapping_generator.py input.json output.json --api-key YOUR_KEY

  # ë°°ì¹˜ ì²˜ë¦¬ (ë””ë ‰í† ë¦¬)
  python mapping_generator.py "data/*.json" output_dir/ --api-key YOUR_KEY --batch

  # ë°°ì¹˜ ì²˜ë¦¬ (íŒŒì¼ ë¦¬ìŠ¤íŠ¸)
  python mapping_generator.py file1.json file2.json file3.json output_dir/ --api-key YOUR_KEY --batch
        """
    )

    parser.add_argument('input', nargs='+', help='ì…ë ¥ JSON íŒŒì¼ (ë‹¨ì¼) ë˜ëŠ” íŒŒì¼ë“¤/íŒ¨í„´ (ë°°ì¹˜)')
    parser.add_argument('output', help='ì¶œë ¥ JSON íŒŒì¼ (ë‹¨ì¼) ë˜ëŠ” ì¶œë ¥ ë””ë ‰í† ë¦¬ (ë°°ì¹˜)')
    parser.add_argument('--api-key', required=True, help='LLM API í‚¤')
    parser.add_argument('--provider', default='anthropic', choices=['anthropic', 'openai'],
                        help='LLM ì œê³µì (ê¸°ë³¸: anthropic)')
    parser.add_argument('--model', help='ì‚¬ìš©í•  ëª¨ë¸ (ê¸°ë³¸: providerë³„ ê¸°ë³¸ ëª¨ë¸)')
    parser.add_argument('--force', action='store_true', help='ê¸°ì¡´ ë§¤í•‘ ë¬´ì‹œí•˜ê³  ì¬ìƒì„±')
    parser.add_argument('--batch', action='store_true', help='ë°°ì¹˜ ì²˜ë¦¬ ëª¨ë“œ')
    parser.add_argument('--continue-on-error', action='store_true', default=True,
                        help='ë°°ì¹˜ ì²˜ë¦¬ ì‹œ ì—ëŸ¬ ë°œìƒí•´ë„ ê³„ì† ì§„í–‰ (ê¸°ë³¸: True)')

    args = parser.parse_args()

    if args.batch:
        # ë°°ì¹˜ ì²˜ë¦¬ ëª¨ë“œ
        data_files = []

        # ì…ë ¥ íŒŒì¼ ë¦¬ìŠ¤íŠ¸ í™•ì¥ (glob íŒ¨í„´ ì§€ì›)
        for pattern in args.input:
            matches = glob.glob(pattern)
            if matches:
                data_files.extend(matches)
            else:
                data_files.append(pattern)  # glob ë§¤ì¹˜ ì—†ìœ¼ë©´ ì›ë³¸ ê²½ë¡œ ì‚¬ìš©

        if not data_files:
            print("âŒ ì—ëŸ¬: ì…ë ¥ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            sys.exit(1)

        print(f"ğŸ“ ë°œê²¬ëœ íŒŒì¼: {len(data_files)}ê°œ")

        # ë°°ì¹˜ ì²˜ë¦¬ ì‹¤í–‰
        results = update_mappings_batch(
            data_files=data_files,
            output_dir=args.output,
            api_key=args.api_key,
            force_regenerate=args.force,
            llm_provider=args.provider,
            model=args.model,
            continue_on_error=args.continue_on_error
        )

        # ì¢…ë£Œ ì½”ë“œ ì„¤ì •
        sys.exit(0 if results['failed'] == 0 else 1)

    else:
        # ë‹¨ì¼ íŒŒì¼ ì²˜ë¦¬ ëª¨ë“œ
        if len(args.input) > 1:
            print("âŒ ì—ëŸ¬: ë‹¨ì¼ íŒŒì¼ ëª¨ë“œì—ì„œëŠ” ì…ë ¥ íŒŒì¼ 1ê°œë§Œ ì§€ì •í•˜ì„¸ìš”.")
            print("  (ì—¬ëŸ¬ íŒŒì¼ ì²˜ë¦¬í•˜ë ¤ë©´ --batch ì˜µì…˜ ì‚¬ìš©)")
            sys.exit(1)

        success = update_mapping_single(
            data_file=args.input[0],
            output_file=args.output,
            api_key=args.api_key,
            force_regenerate=args.force,
            llm_provider=args.provider,
            model=args.model
        )

        sys.exit(0 if success else 1)